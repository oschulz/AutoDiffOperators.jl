var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Modules","page":"API","title":"Modules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:module]","category":"page"},{"location":"api/#Types-and-constants","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:macro, :function]","category":"page"},{"location":"api/#Documentation","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/#AutoDiffOperators.AutoDiffOperators","page":"API","title":"AutoDiffOperators.AutoDiffOperators","text":"AutoDiffOperators\n\nProvides Julia operators that act via automatic differentiation.\n\n\n\n\n\n","category":"module"},{"location":"api/#AutoDiffOperators.FwdRevADSelector","page":"API","title":"AutoDiffOperators.FwdRevADSelector","text":"AutoDiffOperators.FwdRevADSelector{Fwd<:ADSelector,Rev<:ADSelector} <: ADSelector\n\nRepresent an automatic differentiation backend that forwards forward-mode and reverse-mode AD to two separate selectors fwd::ADSelector and rev::ADSelector.\n\nUser code should not instantiate AutoDiffOperators.FwdRevADSelector directly, but use ADSelector(fwd, rev) or ADSelector(fwd = fwd, rev = rev) instead.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.WrappedADSelector","page":"API","title":"AutoDiffOperators.WrappedADSelector","text":"abstract type AutoDiffOperators.WrappedADSelector\n\nSupertype for AD selectors that wrap other AD selectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.ADSelector","page":"API","title":"AutoDiffOperators.ADSelector","text":"const ADSelector = Union{\n    AbstractADType,\n    WrappedADSelector\n}\n\nInstances specify an automatic differentiation backend.\n\nEither a subtype of ADTypes.AbstractADType, or an AD-selector wrapper like AutoDiffOperators.FwdRevADSelector.\n\nIn addition to using instances of AbstractADType directly (e.g.  ADTypes.AutoForwardDiff()), ADSelector (specifically AbstractADType) instances for AD backends can be constructed directly from the backend modules (using default backend parameters):\n\nimport ForwardDiff\n\nADTypes.AutoForwardDiff()\nADSelector(ForwardDiff)\nconvert(ADSelector, ForwardDifsf)\n\nall construct an identical AutoForwardDiff object.\n\nSeparate AD backends for forward- and reverse-mode AD can be specified via ADSelector(fwd_adtype, rev_adtype), e.g.\n\nimport ForwardDiff, Mooncake\n\nADSelector(ADTypes.AutoForwardDiff(), ADTypes.AutoMooncake())\nADSelector(ADSelector(ForwardDiff), ADSelector(Mooncake))\nADSelector(ForwardDiff, Mooncake)\n\nImplementation\n\nADSelector instances can also be constructed from module names, though this should be avoided in end-user code:\n\nADSelector(:ForwardDiff)\nADSelector(Val(:ForwardDiff))\nconvert(ADSelector, :ForwardDiff)\nconvert(ADSelector, Val(:ForwardDiff))\n\nEnd-users should use module objects instead of module name, so that the respective AD backend package must be part of their environment/dependencies.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.forward_adtype","page":"API","title":"AutoDiffOperators.forward_adtype","text":"forward_adtype(ad::ADSelector)::ADTypes.AbstractADType\n\nReturns the forward-mode AD backend selector for ad.\n\nReturns ad itself by default if ad supports forward-mode automatic differentation, or instance of ADTypes.NoAutoDiff if it does not.\n\nMay be specialized for some AD selector types, see FwdRevADSelector, for example.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.gradient_func","page":"API","title":"AutoDiffOperators.gradient_func","text":"gradient_func(f, ad::ADSelector)\n\nReturns a function ∇f that calculates the gradient of f at a given point x, so that ∇f(x) is equivalent to only_gradient(f, x, ad).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.jvp_func","page":"API","title":"AutoDiffOperators.jvp_func","text":"jvp_func(f, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns a function jvp with jvp(z) == J * z.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.mulfunc_operator","page":"API","title":"AutoDiffOperators.mulfunc_operator","text":"AutoDiffOperators.mulfunc_operator(\n    ::Type{OP},\n    ::Type{T}, sz::Dims{2}, ovp, vop,\n    ::Val{sym}, ::Val{herm}, ::Val{posdef}\n) where {OP, T<:Real, sym, herm, posdef}\n\nGenerates a linear operator object of type OP that supports multiplication and with (adjoint) vectors based on a multiplication function ovp and an adjoint multiplication function vop.\n\nAn operator op = mulfunc_operator(OP, T, sz, ovp, vop, Val(sym), ::Val{herm}, ::Val{posdef}) must show show following behavior:\n\nop isa OP\neltype(op) == T\nsize(op) == sz\nop * x_r == ovp(x_r)\nx_l' * op == vop(x_l)\nissymmetric(op) == sym\nishermitian(op) == herm\nisposdef(op) = posdef\n\nwhere x_l and x_r are vectors of size sz[1] and sz[2] respectively.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.only_gradient","page":"API","title":"AutoDiffOperators.only_gradient","text":"only_gradient(f, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns the gradient ∇f(x) of f at x.\n\nSee also with_gradient(f, x, ad).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.only_gradient!","page":"API","title":"AutoDiffOperators.only_gradient!","text":"only_gradient!(f, δx, x::AbstractVector{<:Number}, ad::ADSelector)\n\nFills δx with the ∇f(x) of f at x and returns it.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.only_gradient!!","page":"API","title":"AutoDiffOperators.only_gradient!!","text":"only_gradient!!(f, δx, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns the gradient ∇f(x) of f at x.\n\nδx may or may not be reused/overwritten and returned as ∇f(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.reverse_adtype","page":"API","title":"AutoDiffOperators.reverse_adtype","text":"reverse_adtype(ad::ADSelector)::ADTypes.AbstractADType\n\nReturns the reverse-mode AD backend selector for ad.\n\nReturns ad itself by default if ad supports reverse-mode automatic differentation, or instance of ADTypes.NoAutoDiff if it does not.\n\nMay be specialized for some AD selector types, see FwdRevADSelector, for example.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.similar_onehot","page":"API","title":"AutoDiffOperators.similar_onehot","text":"AutoDiffOperators.similar_onehot(A::AbstractArray, ::Type{T}, n::Integer, i::Integer)\n\nReturn an array similar to A, but with n elements of type T, all set to zero but the i-th element set to one.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.valgrad_func","page":"API","title":"AutoDiffOperators.valgrad_func","text":"valgrad_func(f, ad::ADSelector)\n\nReturns a function f_∇f that calculates the value and gradient of f at given points, so that f_∇f(x) is equivalent to with_gradient(f, x, ad).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.valid_forward_adtype","page":"API","title":"AutoDiffOperators.valid_forward_adtype","text":"valid_forward_adtype(ad::ADSelector)::ADTypes.AbstractADType\n\nSimilar to forward_adtype, but throws an exception if ad doesn't support forward-mode automatic differentiation instead of returning a NoAutoDiff.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.valid_reverse_adtype","page":"API","title":"AutoDiffOperators.valid_reverse_adtype","text":"valid_reverse_adtype(ad::ADSelector)::ADTypes.AbstractADType\n\nSimilar to reverse_adtype, but throws an exception if ad doesn't support reverse-mode automatic differentiation instead of returning a NoAutoDiff.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_floatlike_contents","page":"API","title":"AutoDiffOperators.with_floatlike_contents","text":"AutoDiffOperators.with_floatlike_contents(A::AbstractArray)\n\nIf the elements of A are integer-like, convert them using float, otherwise return A unchanged.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_gradient","page":"API","title":"AutoDiffOperators.with_gradient","text":"with_gradient(f, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns a tuple (f(x), ∇f(x)) with the gradient ∇f(x) of f at x.\n\nSee also with_gradient!!(f, δx, x, ad) for the \"maybe-in-place\" variant of this function.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_gradient!","page":"API","title":"AutoDiffOperators.with_gradient!","text":"with_gradient!(f, δx, x::AbstractVector{<:Number}, ad::ADSelector)\n\nFills δx with the the gradient ∇f(x) of f at x and returns the tuple (f(x), δx).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_gradient!!","page":"API","title":"AutoDiffOperators.with_gradient!!","text":"with_gradient!!(f, δx, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns a tuple (f(x), ∇f(x)) with the gradient ∇f(x) of f at x.\n\nδx may or may not be reused/overwritten and returned as ∇f(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_jacobian","page":"API","title":"AutoDiffOperators.with_jacobian","text":"with_jacobian(f, x::AbstractVector{<:Number}, OP, ad::ADSelector)\n\nReturns a tuple (f(x), J) with a multiplicative Jacobian operator J of type OP.\n\nExample:\n\nusing AutoDiffOperators, LinearMaps\ny, J = with_jacobian(f, x, LinearMap, ad)\ny == f(x)\n_, J_explicit = with_jacobian(f, x, DenseMatrix, ad)\nJ * z_r ≈ J_explicit * z_r\nz_l' * J ≈ z_l' * J_explicit\n\nOP may be LinearMaps.LinearMap (resp. LinearMaps.FunctionMap) or Matrix. Other operator types can be supported by specializing mulfunc_operator for the operator type.\n\nThe default implementation of with_jacobian uses jvp_func and with_vjp_func to implement (adjoint) multiplication of J with (adjoint) vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_jvp","page":"API","title":"AutoDiffOperators.with_jvp","text":"with_jvp(f, x::AbstractVector{<:Number}, z::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns a tuple (f(x), J * z).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_vjp_func","page":"API","title":"AutoDiffOperators.with_vjp_func","text":"with_vjp_func(f, x::AbstractVector{<:Number}, ad::ADSelector)\n\nReturns a tuple (f(x), vjp) with the function vjp(z) ≈ J' * z.\n\n\n\n\n\n","category":"function"},{"location":"LICENSE/#LICENSE","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"LICENSE.md\"))","category":"page"},{"location":"#AutoDiffOperators.jl","page":"Home","title":"AutoDiffOperators.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides multiplicative operators that act via automatic differentiation (AD), as well as additional AD-related functionality.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For Jacobians, this package provides the function with_jacobian. It can return implicit Jacobian operators (e.g. a LinearMap/FunctionMap or similar) or explicit Jacobian operators (i.e. a DenseMatrix). Lower-level functions with_jvp, jvp_func, with_vjp_func are provided as well.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In respect to gradients, different Julia algorithm packages require function and gradient calculation to be passed in a different fashion. AutoDiffOperators provides","category":"page"},{"location":"","page":"Home","title":"Home","text":"valgrad_func(f, ad::ADSelector): returns f_∇f with y, δx = f_∇f(x).\ngradient_func(f, ad::ADSelector): returns ∇f with δx = ∇f(x).","category":"page"},{"location":"","page":"Home","title":"Home","text":"to cover several popular options. AutoDiffOperators also provides the direct gradient functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"with_gradient, with_gradient!, with_gradient!!\nonly_gradient, only_gradient!, only_gradient!!","category":"page"},{"location":"","page":"Home","title":"Home","text":"AD-backends are specified via subtypes of ADSelector, which includes ADTypes.AbstractADType. In addition to using subtypes of AbstractADType directly, you can use ADSelector(SomeADModule) (e.g. ADSelector(ForwardDiff)) to select a backend with default options. Separate backends for forward and reverse mode AD can be specified via ADSelector(fwd_adtype, rev_adtype).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples for valid ADSelector choices:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ADTypes.AutoForwardDiff()\nADSelector(ForwardDiff)\nADSelector(ADTypes.AutoForwardDiff(), ADTypes.AutoMooncake())\nADSelector(ADSelector(ForwardDiff), ADSelector(Mooncake))\nADSelector(ForwardDiff, Mooncake)","category":"page"},{"location":"","page":"Home","title":"Home","text":"AutoDiffOperators re-exports AbstractADType and NoAutoDiff from ADTypes, so that algorithm packages that use AutoDiffOperators may not need to also depend on ADTypes explicitly.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AutoDiffOperators uses DifferentiationInterface internally to interact with the various Julia AD backend packages, adding some specializations and optimizations for type stability and performance. Which backend(s) will perform best for a given use case will depend on the target function and the argument size, as well as the application (J*z vs. z'*J and gradient calculation) and the compute device (CPU vs. GPU). Please see the documentation of the individual AD backend packages for details.","category":"page"}]
}
