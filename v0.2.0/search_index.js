var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Modules","page":"API","title":"Modules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:module]","category":"page"},{"location":"api/#Types-and-constants","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Order = [:macro, :function]","category":"page"},{"location":"api/#Documentation","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [AutoDiffOperators]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#AutoDiffOperators.AutoDiffOperators","page":"API","title":"AutoDiffOperators.AutoDiffOperators","text":"AutoDiffOperators\n\nProvides Julia operators that act via automatic differentiation.\n\n\n\n\n\n","category":"module"},{"location":"api/#AutoDiffOperators.DiffIfAD","page":"API","title":"AutoDiffOperators.DiffIfAD","text":"DiffIfAD{Fwd<:ADSelector,Rev<:ADSelector} <: ADSelector\n\nUses DifferentiationInterfac to interface with an AD-backend.\n\nConstructor: DiffIfAD(backend::ADTypes.AbstractADType)\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.FwdRevADSelector","page":"API","title":"AutoDiffOperators.FwdRevADSelector","text":"AutoDiffOperators.FwdRevADSelector{Fwd<:ADSelector,Rev<:ADSelector} <: ADSelector\n\nRepresent an automatic differentiation backend that forwards forward-mode and reverse-mode AD to two separate selectors fwd::ADSelector and rev::ADSelector.\n\nUser code should not instantiate AutoDiffOperators.FwdRevADSelector directly, but use ADSelector(fwd, rev) or ADSelector(fwd = fwd, rev = rev) instead.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.WrappedADSelector","page":"API","title":"AutoDiffOperators.WrappedADSelector","text":"abstract type AutoDiffOperators.WrappedADSelector\n\nSupertype for AD selectors that wrap other AD selectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.ADSelector","page":"API","title":"AutoDiffOperators.ADSelector","text":"const ADSelector = Union{\n    ADTypes.AbstractADType,\n    WrappedADSelector\n}\n\nInstances speficy an automatic differentiation backend.\n\nEither a subtype of [ADTypes.AbstractADType]](https://github.com/SciML/ADTypes.jl), or an AD-selector wrapper like AutoDiffOperators.FwdRevADSelector.\n\nAutoDiffOperators currently provides it's own implementations for following AD-selectors: AutoForwardDiff(), AutoFiniteDifferences(), AutoZygote() and AutoEnzyme().\n\nSome operations that specifically require forward-mode or reverse-mode AD will only accept a subset of these backends though.\n\nAlternatively, DifferentiationInterface` can be used to interface with various AD-backends, by using DiffIfAD(backend::ADTypes.AbstractADType) as the AD-selector.\n\nImplementation\n\nThe following functions must be specialized for subtypes of ADSelector: with_jvp, with_vjp_func.\n\nAutoDiffOperators.supports_structargs should be specialized if applicable.\n\nA default implementation is provided for with_gradient, but specialized implementations may often be more performant.\n\nSelector types that delegate forward and/or reverse-mode AD to other selector types resp AD-backends should also specialize forward_ad_selector and reverse_ad_selector.\n\n\n\n\n\n","category":"type"},{"location":"api/#AutoDiffOperators.forward_ad_selector","page":"API","title":"AutoDiffOperators.forward_ad_selector","text":"forward_ad_selector(ad::ADSelector)::ADSelector\n\nReturns the forward-mode AD backen selector for ad.\n\nReturns ad itself by default. Also see FwdRevADSelector.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.gradient!_func","page":"API","title":"AutoDiffOperators.gradient!_func","text":"gradient!_func(f, ad::ADSelector)\n\nReturns a tuple (f, ∇f!) with the functions f(x) and ∇f!(δx, x).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.gradient_func","page":"API","title":"AutoDiffOperators.gradient_func","text":"gradient_func(f, ad::ADSelector)\n\nReturns a tuple (f, ∇f) with the functions f(x) and ∇f(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.jvp_func-Tuple{Any, Any, ADSelector}","page":"API","title":"AutoDiffOperators.jvp_func","text":"jvp_func(f, x, ad::ADSelector)\n\nReturns a function jvp with jvp(z) == J * z.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoDiffOperators.mulfunc_operator","page":"API","title":"AutoDiffOperators.mulfunc_operator","text":"AutoDiffOperators.mulfunc_operator(\n    ::Type{OP},\n    ::Type{T}, sz::Dims{2}, ovp, vop,\n    ::Val{sym}, ::Val{herm}, ::Val{posdef}\n) where {OP, T<:Real, sym, herm, posdef}\n\nGenerates a linear operator object of type OP that supports multiplication and with (adjoint) vectors based on a multiplication function ovp and an adjoint multiplication function vop.\n\nAn operator op = mulfunc_operator(OP, T, sz, ovp, vop, Val(sym), ::Val{herm}, ::Val{posdef}) must show show following behavior:\n\nop isa OP\neltype(op) == T\nsize(op) == sz\nop * x_r == ovp(x_r)\nx_l' * op == vop(x_l)\nissymmetric(op) == sym\nishermitian(op) == herm\nisposdef(op) = posdef\n\nwhere x_l and x_r are vectors of size sz[1] and sz[2] respectively.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.only_gradient","page":"API","title":"AutoDiffOperators.only_gradient","text":"only_gradient(f, x, ad::ADSelector)\n\nReturns the gradient ∇f(x) of f at x.\n\nSee also with_gradient(f, x, ad).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.reverse_ad_selector","page":"API","title":"AutoDiffOperators.reverse_ad_selector","text":"reverse_ad_selector(ad::ADSelector)::ADSelector\n\nReturns the reverse-mode AD backen selector for ad.\n\nReturns ad itself by default. Also see FwdRevADSelector.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.similar_onehot","page":"API","title":"AutoDiffOperators.similar_onehot","text":"AutoDiffOperators.similar_onehot(A::AbstractArray, ::Type{T}, n::Integer, i::Integer)\n\nReturn an array similar to A, but with n elements of type T, all set to zero but the i-th element set to one.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.supports_structargs","page":"API","title":"AutoDiffOperators.supports_structargs","text":"AutoDiffOperators.supports_structargs(ad::ADSelector)::Boolean\n\nReturns true if ad supports structured function arguments or false if ad only supports vectors of real numbers.\n\nSince ad may use different backends for forward- and reverse-mode AD, use supports_structargs(forward_ad_selector(ad)) and  supports_structargs(reverse_ad_selector(ad)) to check if ad supports structured arguments for the desired operation.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.valgrad_func","page":"API","title":"AutoDiffOperators.valgrad_func","text":"valgrad_func(f, ad::ADSelector)\n\nReturns a function f_∇f that calculates the value and gradient of f at given points, so that f_∇f(x) is equivalent to with_gradient(f, x, ad).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_floatlike_contents","page":"API","title":"AutoDiffOperators.with_floatlike_contents","text":"AutoDiffOperators.with_floatlike_contents(A::AbstractArray)\n\nIf the elements of A are integer-like, convert them using float, otherwise return A unchanged.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_gradient","page":"API","title":"AutoDiffOperators.with_gradient","text":"with_gradient(f, x, ad::ADSelector)\n\nReturns a tuple (f(x), ∇f(x)) with the gradient ∇f(x) of f at x.\n\nSee also with_gradient!!(f, δx, x, ad) for the \"maybe-in-place\" variant of this function.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_gradient!!","page":"API","title":"AutoDiffOperators.with_gradient!!","text":"with_gradient!!(f, δx, x, ad::ADSelector)\n\nReturns a tuple (f(x), ∇f(x)) with the gradient ∇f(x)offatx`.\n\nδx may or may not be reused/overwritten and returned as ∇f(x).\n\nThe default implementation falls back to with_gradient(f, x, ad), subtypes of ADSelector may specialized with_gradient!! to provide more efficient implementations.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_jacobian","page":"API","title":"AutoDiffOperators.with_jacobian","text":"with_jacobian(f, x, OP, ad::ADSelector)\n\nReturns a tuple (f(x), J) with a multiplicative Jabobian operator J of type OP.\n\nExample:\n\nusing AutoDiffOperators, LinearMaps\ny, J = with_jacobian(f, x, LinearMap, ad)\ny == f(x)\n_, J_explicit = with_jacobian(f, x, Matrix, ad)\nJ * z_r ≈ J_explicit * z_r\nz_l' * J ≈ z_l' * J_explicit\n\nOP may be LinearMaps.LinearMap (resp. LinearMaps.FunctionMap) or Matrix. Other operator types can be supported by specializing AutoDiffOperators.mulfunc_operator for the operator type.\n\nThe default implementation of with_jacobian uses jvp_func and with_vjp_func to implement (adjoint) multiplication of J with (adjoint) vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_jvp","page":"API","title":"AutoDiffOperators.with_jvp","text":"with_jvp(f, x, z, ad::ADSelector)\n\nReturns a tuple (f(x), J * z).\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoDiffOperators.with_vjp_func","page":"API","title":"AutoDiffOperators.with_vjp_func","text":"with_vjp_func(f, x, ad::ADSelector)\n\nReturns a tuple (f(x), vjp) with the function vjp(z) ≈ J' * z.\n\n\n\n\n\n","category":"function"},{"location":"LICENSE/#LICENSE","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"LICENSE.md\"))","category":"page"},{"location":"#AutoDiffOperators.jl","page":"Home","title":"AutoDiffOperators.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides multiplicative operators that act via automatic differentiation (AD), as well as additional AD-related functionality.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AD-backends are specified via subtypes of ADSelector, which includes ADTypes.AbstractADType. separate backends for forward and reverse mode AD can be specified if desired.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The main functions are with_gradient and with_jacobian. The central lower-level functions are with_jvp and with_vjp_func. Jacobian operators can be implicit (e.g. a LinearMap/FunctionMap or similar) or explixit (i.e. a Matrix).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Different Julia packages require function and gradient calculation to be passed in a different fashion. AutoDiffOperators provides","category":"page"},{"location":"","page":"Home","title":"Home","text":"valgrad_func(f, ad::ADSelector): generates f_∇f with y, δx = f_∇f(x).\ngradient_func(f, ad::ADSelector): generates ∇f with δx = ∇f(x).\ngradient!_func(f, ad::ADSelector): generates ∇f! with δx === ∇f!(δx, x).","category":"page"},{"location":"","page":"Home","title":"Home","text":"to cover several popular options.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AutoDiffOperators natively supports the following automatic differentiation packages as backends:","category":"page"},{"location":"","page":"Home","title":"Home","text":"FiniteDifferences, selected via ADTypes.AutoFiniteDifferences() or ADSelector(FiniteDifferences).\nForwardDiff, selected via ADTypes.AutoForwardDiff() or ADSelector(ForwardDiff).\nZygote, selected via ADTypes.AutoZygote() or ADSelector(Zygote).\nEnzyme, selected via ADTypes.AutoEnzyme() or ADSelector(Enzyme).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, DifferentiationInterface` can be used to interface with various AD-backends, by using DiffIfAD(backend::ADTypes.AbstractADType) as the AD-selector.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AutoDiffOperators may not support all options and funcionalities of these AD packages. Also, most of them have some limitations on which code constructs in the target function and which function argument types they support. Which backend(s) will perform best will depend on the target function and the argument size, as well as the application (J*z vs. z'*J and gradient calculation) and the compute device (CPU vs. GPU). Please see the documentation of the individual AD packages linked above for more details on their capabilities.","category":"page"}]
}
